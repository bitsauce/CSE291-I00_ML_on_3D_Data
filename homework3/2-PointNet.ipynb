{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcus/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pyntcloud import PyntCloud\n",
    "from modelnet import * # Includes the dataloader + data augmentation functions\n",
    "import utils\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ModelNet 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the ModelNet data loader\n",
    "modelnet = ModelNet(shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network with tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PointNet](PointNet-Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Point cloud placeholder -> [batch_size x num_points x 3]\n",
    "points = tf.placeholder(tf.float32, [None, modelnet.num_points, 3])\n",
    "labels = tf.placeholder(tf.float32, [None, modelnet.num_categories])\n",
    "batch_norm_decay = tf.placeholder(tf.float32)\n",
    "dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# Initializes the bias to identity\n",
    "def bias_identity_init(shape, dtype, partition_info):\n",
    "    rows = np.sqrt(shape[0]).astype(dtype=np.int32)\n",
    "    return tf.reshape(tf.eye(rows, dtype=dtype), [-1])\n",
    "\n",
    "def transformation_network(inputs, output_size):\n",
    "    # Increase num features -> MLP(64, 128, 1024)\n",
    "    feat = tf.layers.conv1d(inputs, filters=64, kernel_size=1, strides=1, activation=tf.nn.relu)   # [batch_size x num_points x 64]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    feat = tf.layers.conv1d(feat, filters=128, kernel_size=1, strides=1, activation=tf.nn.relu)  # [batch_size x num_points x 128]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    feat = tf.layers.conv1d(feat, filters=1024, kernel_size=1, strides=1, activation=tf.nn.relu) # [batch_size x num_points x 1024]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "\n",
    "    # Extract global features -> maxpool\n",
    "    feat = tf.reduce_max(feat, 1) # [batch_size x 1024]\n",
    "\n",
    "    # Generate predictions -> FC(512, 256, num_categories)\n",
    "    feat = tf.layers.dense(feat, units=512, activation=tf.nn.relu) # [batch_size x 512]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    feat = tf.layers.dense(feat, units=256, activation=tf.nn.relu) # [batch_size x 256]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    return tf.layers.dense(feat, units=output_size, activation=None,\n",
    "                           bias_initializer=bias_identity_init,\n",
    "                           kernel_initializer=tf.zeros_initializer) # [batch_size x output_size]\n",
    "\n",
    "def final_transformation_network(inputs, output_size):\n",
    "    # Increase num features -> MLP(64, 128, 1024)\n",
    "    feat = tf.layers.conv1d(inputs, filters=64, kernel_size=1, strides=1, activation=tf.nn.relu)   # [batch_size x num_points x 64]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    feat = tf.layers.conv1d(feat, filters=128, kernel_size=1, strides=1, activation=tf.nn.relu)  # [batch_size x num_points x 128]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    feat = tf.layers.conv1d(feat, filters=1024, kernel_size=1, strides=1, activation=tf.nn.relu) # [batch_size x num_points x 1024]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "\n",
    "    # Extract global features -> maxpool\n",
    "    feat = tf.reduce_max(feat, 1) # [batch_size x 1024]\n",
    "\n",
    "    # Generate predictions -> FC(512, 256, num_categories)\n",
    "    feat = tf.layers.dense(feat, units=512, activation=tf.nn.relu) # [batch_size x 512]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    feat = tf.layers.dense(feat, units=256, activation=tf.nn.relu) # [batch_size x 256]\n",
    "    feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "    feat = tf.layers.dropout(feat, rate=dropout_rate) # Dropout with keep rate 0.7 = 1.0 - 0.3\n",
    "    return tf.layers.dense(feat, units=output_size, activation=None) # [batch_size x output_size]\n",
    "\n",
    "# Transform input\n",
    "transformation_matrix = tf.reshape(transformation_network(points, 3 * 3), [-1, 3, 3])\n",
    "feat = tf.matmul(points, transformation_matrix)\n",
    "\n",
    "# Points to features -> MLP(64, 64)\n",
    "feat = tf.layers.conv1d(feat, filters=64, kernel_size=1, strides=1, activation=tf.nn.relu) # [batch_size x num_points x 64]\n",
    "feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "feat = tf.layers.conv1d(feat, filters=64, kernel_size=1, strides=1, activation=tf.nn.relu) # [batch_size x num_points x 64]\n",
    "feat = tf.contrib.layers.batch_norm(feat, decay=batch_norm_decay)\n",
    "\n",
    "# Transform features\n",
    "transformation_matrix = tf.reshape(transformation_network(feat, 64 * 64), [-1, 64, 64])\n",
    "\n",
    "# Add Frobenious norm of feature transform matrix to loss\n",
    "Lreg = tf.nn.l2_loss(tf.eye(64) - tf.matmul(transformation_matrix, tf.transpose(transformation_matrix, perm=[0, 2, 1])))\n",
    "feat = tf.matmul(feat, transformation_matrix)\n",
    "\n",
    "# Features to classification\n",
    "feat = final_transformation_network(feat, modelnet.num_categories)\n",
    "\n",
    "# Cross entropy softmax loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=feat, labels=labels)) + Lreg * 0.001\n",
    "\n",
    "# Get predicted classes\n",
    "predictions = tf.argmax(tf.nn.softmax(feat), axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_labels = tf.equal(predictions, tf.argmax(labels, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_labels, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for learning rate (used in training loop)\n",
    "learning_rate = tf.placeholder(tf.float32, [], name=\"learning_rate\")\n",
    "\n",
    "# Initialize AdamOptimizer\n",
    "optim = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Create session\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(device_count = {'GPU': 0}))\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee36abfbe74e5a873a22ad6f6b0432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Label, Box\n",
    "from IPython.display import display\n",
    "\n",
    "statusLabel = Label(\"\")\n",
    "display(Box([statusLabel]))\n",
    "\n",
    "# Make batches to train\n",
    "batch_size = 32\n",
    "current_epoch = 0\n",
    "current_learning_rate = 0.001\n",
    "current_bn_decay = 0.5\n",
    "current_loss = 0\n",
    "current_test_accuracy = 0\n",
    "num_iter = 0\n",
    "start_time = time.time()\n",
    "epoch_time = 0\n",
    "\n",
    "# Try to load model file if epoch is specified\n",
    "if current_epoch != 0:\n",
    "    saver = tf.train.import_meta_graph(\"models/PointNet-%i.meta\" % current_epoch)\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"./models\"))\n",
    "    current_learning_rate /= np.power(2, current_epoch // 20)\n",
    "    stats = load_object(\"stats/PointNet_Vanilla.stats\")\n",
    "else:\n",
    "    saver = tf.train.Saver()\n",
    "    stats = { key: [] for key in [\"loss\", \"accuracy\"] }\n",
    "\n",
    "while True:\n",
    "    # If epoch of training data is complete\n",
    "    if modelnet.train.is_epoch_complete():\n",
    "        # Measure time\n",
    "        epoch_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Avg loss\n",
    "        current_loss /= num_iter\n",
    "        \n",
    "        # Calculate accuracy on test set\n",
    "        num_iter = 0\n",
    "        current_test_accuracy = 0\n",
    "        epoch_complete = False\n",
    "        while not modelnet.test.is_epoch_complete():\n",
    "            batch_points, batch_labels = modelnet.test.next_batch(batch_size)\n",
    "            current_test_accuracy += sess.run(accuracy, feed_dict={points: batch_points,\n",
    "                                                                   labels: batch_labels,\n",
    "                                                                   dropout_rate: 0.0})\n",
    "            num_iter += 1\n",
    "        current_test_accuracy /= num_iter\n",
    "            \n",
    "        \n",
    "        # Save best model\n",
    "        if current_epoch == 0 or current_test_accuracy > np.max(stats[\"accuracy\"]):\n",
    "            saver.save(sess, \"models/PointNet-best\")\n",
    "            save_object(stats, \"stats/PointNet.stats\")\n",
    "            \n",
    "        # Increase epoch count\n",
    "        current_epoch += 1\n",
    "        \n",
    "        # Append some stats\n",
    "        stats[\"loss\"].append(current_loss)\n",
    "        stats[\"accuracy\"].append(current_test_accuracy)\n",
    "        \n",
    "        # Every 20th epoch, halve the learning rate\n",
    "        if current_epoch % 20 == 0:\n",
    "            # Save the model\n",
    "            saver.save(sess, \"models/PointNet\", global_step=current_epoch)\n",
    "            save_object(stats, \"stats/PointNet.stats\")\n",
    "            \n",
    "            # Halve the learning rate\n",
    "            current_learning_rate /= 2.0\n",
    "            \n",
    "            # Interpolate decay rate from 0.5 to 0.99 over 80 epochs\n",
    "            current_bn_decay = lerp(0.5, 0.99, np.min([(current_epoch // 20) / 4, 1.0]))\n",
    "        \n",
    "        # Display training status\n",
    "        statusSting  = \"[Epoch %i] \" % current_epoch\n",
    "        statusSting += \"Time: %im %is; \" % (epoch_time // 60, epoch_time % 60)\n",
    "        statusSting += \"Loss: %f; \" % current_loss\n",
    "        statusSting += \"Learning rate: %f; \" % current_learning_rate\n",
    "        statusSting += \"BN decay rate: %f; \" % current_bn_decay\n",
    "        statusSting += \"Accuracy: %f; \" % current_test_accuracy\n",
    "        statusSting += \"--- \";\n",
    "        statusSting += \"Current best: [Epoch %i] \" % np.argmax(stats[\"accuracy\"])\n",
    "        statusSting += \"Accuracy %f; \" % np.max(stats[\"accuracy\"])\n",
    "        statusLabel.value = statusSting\n",
    "        \n",
    "        # Reset loss\n",
    "        current_loss = 0\n",
    "        num_iter = 0\n",
    "    \n",
    "    # Get next batch\n",
    "    batch_points, batch_labels = modelnet.train.next_batch(batch_size)\n",
    "    \n",
    "    # Rotate 10% of the time\n",
    "    if np.random.rand() < 0.1:\n",
    "        batch_points = rotate(batch_points, np.pi * np.random.choice([1.0 / 2.0, 1.0, 3.0 / 2.0]))\n",
    "    \n",
    "    # Jitter 25% of the time\n",
    "    if np.random.rand() < 0.25:\n",
    "        batch_points = jitter(batch_points, 0.0, 0.02)\n",
    "    \n",
    "    # Do training\n",
    "    current_loss += sess.run([optim, loss], feed_dict={points: batch_points,\n",
    "                                                       labels: batch_labels,\n",
    "                                                       learning_rate: current_learning_rate,\n",
    "                                                       batch_norm_decay: current_bn_decay,\n",
    "                                                       dropout_rate: 0.3})[1]\n",
    "    num_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best result\n",
    "[Epoch 428] Time: 2m 53s; Loss: 0.180971; Learning rate: 0.000000; BN decay rate: 0.990000; Accuracy: 0.857372; --- Current best: [Epoch 204] Accuracy 0.870994;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
